{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install if needed: pip install yfinance pandas numpy\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"FTSE100_raw.csv\")\n",
    "print(data.shape)\n",
    "data = pd.read_csv(\"DAX.csv\")\n",
    "print(data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Date', 'Price', 'Open', 'High', 'Low', 'Vol.', 'Change %'], dtype='object')\n",
      "            Date     Price      Open      High       Low     Vol. Change %\n",
      "0     12/29/2023  7,733.24  7,722.74  7,746.91  7,719.02  225.64M    0.14%\n",
      "1     12/28/2023  7,722.74  7,724.95  7,745.99  7,708.74  316.82M   -0.03%\n",
      "2     12/27/2023  7,724.95  7,697.51  7,759.74  7,697.51  409.46M    0.36%\n",
      "3     12/22/2023  7,697.51  7,694.73  7,715.21  7,676.43  320.56M    0.04%\n",
      "4     12/21/2023  7,694.73  7,715.68  7,715.68  7,668.41  562.95M   -0.27%\n",
      "...          ...       ...       ...       ...       ...      ...      ...\n",
      "2267  01/08/2015  6,569.96  6,419.83  6,580.82  6,419.83  910.04M    2.34%\n",
      "2268  01/07/2015  6,419.83  6,366.51  6,459.74  6,366.51  709.50M    0.84%\n",
      "2269  01/06/2015  6,366.51  6,417.16  6,452.66  6,328.59  793.26M   -0.79%\n",
      "2270  01/05/2015  6,417.16  6,547.80  6,576.74  6,404.49  750.52M   -2.00%\n",
      "2271  01/02/2015  6,547.80  6,566.09  6,607.89  6,510.60  378.93M   -0.28%\n",
      "\n",
      "[2272 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "#start_date = \"2015-01-01\"\n",
    "#end_date = \"2023-12-31\"\n",
    "data_raw = pd.read_csv(\"FTSE100_raw.csv\")\n",
    "\n",
    "print(data_raw.columns)\n",
    "print(data_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_german_float(value):\n",
    "    if isinstance(value, str):\n",
    "        # Remove periods (thousands separator)\n",
    "        cleaned_value = value.replace('.', '')\n",
    "        # Replace comma (decimal separator) with period\n",
    "        cleaned_value = cleaned_value.replace(',', '.')\n",
    "        try:\n",
    "            return float(cleaned_value)\n",
    "        except ValueError:\n",
    "            return np.nan # Or handle other errors as needed\n",
    "    return value # Return as is if not a string (e.g., already a number or NaN)\n",
    "data_raw['Price'] = data_raw['Price'].apply(convert_german_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['LogReturn', 'Target'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4c/gx6ft8qx2c90jb6mwvlx3gcw0000gn/T/ipykernel_81881/1590925309.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data.dropna(inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------\n",
    "# STEP 2: Clean and enrich the data\n",
    "# -----------------------------------\n",
    "# First compute daily log return\n",
    "data_raw[\"LogReturn\"] = np.log(data_raw[\"Price\"] / data_raw[\"Price\"].shift(1))\n",
    "\n",
    "# Define the prediction target: \"HORIZON\"-day forward cumulative return\n",
    "HORIZON = 5 # cumulative return of 5 days \n",
    "data_raw[\"Target\"] = data_raw[\"LogReturn\"].rolling(window=HORIZON).sum().shift(-HORIZON)\n",
    "# Keep only date, log return and target\n",
    "data = data_raw[[\"LogReturn\", \"Target\"]]\n",
    "print(data.columns)\n",
    "\n",
    "# Drop the first row with NaN return (from shift operation)\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4c/gx6ft8qx2c90jb6mwvlx3gcw0000gn/T/ipykernel_81881/2155772352.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[f\"Lag_{lag}\"] = data[\"LogReturn\"].shift(lag)\n",
      "/var/folders/4c/gx6ft8qx2c90jb6mwvlx3gcw0000gn/T/ipykernel_81881/2155772352.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[f\"Lag_{lag}\"] = data[\"LogReturn\"].shift(lag)\n",
      "/var/folders/4c/gx6ft8qx2c90jb6mwvlx3gcw0000gn/T/ipykernel_81881/2155772352.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[f\"Lag_{lag}\"] = data[\"LogReturn\"].shift(lag)\n",
      "/var/folders/4c/gx6ft8qx2c90jb6mwvlx3gcw0000gn/T/ipykernel_81881/2155772352.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[f\"Lag_{lag}\"] = data[\"LogReturn\"].shift(lag)\n",
      "/var/folders/4c/gx6ft8qx2c90jb6mwvlx3gcw0000gn/T/ipykernel_81881/2155772352.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[f\"Lag_{lag}\"] = data[\"LogReturn\"].shift(lag)\n",
      "/var/folders/4c/gx6ft8qx2c90jb6mwvlx3gcw0000gn/T/ipykernel_81881/2155772352.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data.dropna(inplace=True)  # drop rows with NaNs introduced by lagging\n",
      "/var/folders/4c/gx6ft8qx2c90jb6mwvlx3gcw0000gn/T/ipykernel_81881/2155772352.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data.drop(columns=[\"LogReturn\"], inplace=True) # LogReturn no more needed\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------\n",
    "# STEP 3: Create lag features\n",
    "# -----------------------------------\n",
    "N_LAGS = 5\n",
    "for lag in range(1, N_LAGS + 1):\n",
    "    data[f\"Lag_{lag}\"] = data[\"LogReturn\"].shift(lag)\n",
    "\n",
    "data.dropna(inplace=True)  # drop rows with NaNs introduced by lagging\n",
    "\n",
    "data.drop(columns=[\"LogReturn\"], inplace=True) # LogReturn no more needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Date    Target     Lag_1     Lag_2     Lag_3     Lag_4     Lag_5  \\\n",
      "6    2023-12-19 -0.012550  0.002719 -0.000361 -0.003558  0.000286 -0.001359   \n",
      "7    2023-12-18 -0.009181 -0.010115  0.002719 -0.000361 -0.003558  0.000286   \n",
      "8    2023-12-15 -0.002893 -0.003088 -0.010115  0.002719 -0.000361 -0.003558   \n",
      "9    2023-12-14 -0.017842 -0.005019 -0.003088 -0.010115  0.002719 -0.000361   \n",
      "10   2023-12-13 -0.004389  0.009539 -0.005019 -0.003088 -0.010115  0.002719   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "2262 2015-01-15  0.010893 -0.005369 -0.005236 -0.016173 -0.010143 -0.005312   \n",
      "2263 2015-01-14  0.004898 -0.007892 -0.005369 -0.005236 -0.016173 -0.010143   \n",
      "2264 2015-01-13 -0.027222 -0.017121 -0.007892 -0.005369 -0.005236 -0.016173   \n",
      "2265 2015-01-12 -0.013045  0.023780 -0.017121 -0.007892 -0.005369 -0.005236   \n",
      "2266 2015-01-09  0.007152 -0.006253  0.023780 -0.017121 -0.007892 -0.005369   \n",
      "\n",
      "      day_of_week  month  day_of_month  \n",
      "6               1     12            19  \n",
      "7               0     12            18  \n",
      "8               4     12            15  \n",
      "9               3     12            14  \n",
      "10              2     12            13  \n",
      "...           ...    ...           ...  \n",
      "2262            3      1            15  \n",
      "2263            2      1            14  \n",
      "2264            1      1            13  \n",
      "2265            0      1            12  \n",
      "2266            4      1             9  \n",
      "\n",
      "[2261 rows x 10 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4c/gx6ft8qx2c90jb6mwvlx3gcw0000gn/T/ipykernel_81881/1496232773.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[\"day_of_week\"] = data[\"Date\"].dt.dayofweek   # Monday = 0, Sunday = 6\n",
      "/var/folders/4c/gx6ft8qx2c90jb6mwvlx3gcw0000gn/T/ipykernel_81881/1496232773.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[\"month\"] = data[\"Date\"].dt.month             # 1 to 12\n",
      "/var/folders/4c/gx6ft8qx2c90jb6mwvlx3gcw0000gn/T/ipykernel_81881/1496232773.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[\"day_of_month\"] = data[\"Date\"].dt.day        # 1 to 31\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------\n",
    "# STEP 4: Add temporal features\n",
    "# -----------------------------------\n",
    "data.insert(0, \"Date\", pd.to_datetime(data_raw[\"Date\"], format=\"%m/%d/%Y\"))\n",
    "data[\"day_of_week\"] = data[\"Date\"].dt.dayofweek   # Monday = 0, Sunday = 6\n",
    "data[\"month\"] = data[\"Date\"].dt.month             # 1 to 12\n",
    "data[\"day_of_month\"] = data[\"Date\"].dt.day        # 1 to 31\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"FTSE100_processed.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# STEP 5: Define training set andcommon test set (latest 20%)\n",
    "# -----------------------------------\n",
    "full_train_size = int(len(data) * 0.8)\n",
    "train_data_only = data.iloc[:full_train_size].copy()\n",
    "common_test = data.iloc[full_train_size:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# STEP 6: Simulate 1 single agent and 3 federated agents\n",
    "# -----------------------------------\n",
    "agent_data = {}\n",
    "agent_data[\"SingleAgent_Train\"] = train_data_only.copy()\n",
    "agent_data[\"SingleAgent_Test\"] = common_test.copy()\n",
    "\n",
    "# Without shuffling: split for federated agents\n",
    "federated_data = train_data_only.copy()\n",
    "splits = np.array_split(federated_data, 3)\n",
    "\n",
    "# With shuffling: shuffle and split for federated agents\n",
    "#shuffled = train_data_only.sample(frac=1, random_state=4)\n",
    "#splits = np.array_split(shuffled, 3)\n",
    "\n",
    "# Assign to agents\n",
    "for i, df in enumerate(splits):\n",
    "    agent_data[f\"Agent_{i+1}_Train\"] = df.sort_index()\n",
    "    agent_data[f\"Agent_{i+1}_Test\"] = common_test.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# STEP 7: Preview the datasets\n",
    "# -----------------------------------\n",
    "for name, df in agent_data.items():\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(df.head())\n",
    "    print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install if needed: pip install tensorflow scikit-learn matplotlib\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# STEP 1: Features and target column names\n",
    "# -----------------------------------\n",
    "feature_cols = [f\"Lag_{i}\" for i in range(1, 6)] + [\"day_of_week\", \"month\", \"day_of_month\"]\n",
    "target_col = \"Target\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# STEP 2: Function to build the model\n",
    "# -----------------------------------\n",
    "def build_model(input_dim):\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(150, activation='relu', input_shape=(input_dim,)),\n",
    "        layers.Dense(10, activation='relu'),\n",
    "        layers.Dense(1)  # Linear output for regression\n",
    "    ])\n",
    "    model.compiale(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# STEP 3: Plot training loss and MAE\n",
    "# -----------------------------------\n",
    "def plot_training(history, title):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(history.history['loss'], label='MSE')\n",
    "    plt.plot(history.history['mae'], label='MAE')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# STEP 4: Function to train the model\n",
    "# -----------------------------------\n",
    "def train_model(train_df, label=\"\"):\n",
    "    X = train_df[feature_cols].values\n",
    "    y = train_df[target_col].values\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X) # Normalization\n",
    "\n",
    "    model = build_model(X.shape[1])\n",
    "    print(f\"\\nTraining model on {label}...\")\n",
    "    history = model.fit(X_scaled, y, epochs=50, batch_size=32, verbose=1)\n",
    "\n",
    "    plot_training(history, title=f\"{label} Training Metrics\")\n",
    "    return model, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# STEP 5: Function to evaluate the model\n",
    "# -----------------------------------\n",
    "def evaluate_model(model, scaler, test_df):\n",
    "    X_test = scaler.transform(test_df[feature_cols].values)\n",
    "    y_true = test_df[target_col].values\n",
    "    y_pred = model.predict(X_test).flatten()\n",
    "\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "    print(f\"Test MSE: {mse:.6f}, MAE: {mae:.6f}\")\n",
    "    return mse, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# STEP 6: Single-agent model \n",
    "# -----------------------\n",
    "single_model, single_scaler = train_model(agent_data[\"SingleAgent_Train\"], label=\"SingleAgent\")\n",
    "evaluate_model(single_model, single_scaler, agent_data[\"SingleAgent_Test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# STEP 7: Per-agent models\n",
    "# -----------------------\n",
    "agent_models = {}\n",
    "for i in range(1, 4):\n",
    "    agent_name = f\"Agent_{i}_Train\"\n",
    "    test_name = f\"Agent_{i}_Test\"\n",
    "    model, scaler = train_model(agent_data[agent_name], label=agent_name)\n",
    "    evaluate_model(model, scaler, agent_data[test_name])\n",
    "    agent_models[agent_name] = {\"model\": model, \"scaler\": scaler}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
